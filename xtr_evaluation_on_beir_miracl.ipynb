{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpDZgJORkM0e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG9Qd6gwQz3E"
      },
      "source": [
        "# XTR Evaluation on BEIR \u0026 MIRACL\n",
        "\n",
        "This notebook provides how to load and evaluate [XTR (or mXTR)](https://arxiv.org/abs/2304.01982) on [BEIR](https://arxiv.org/abs/2104.08663) and [MIRACL](https://arxiv.org/abs/2210.09984). More details of XTR are available in the [paper](https://arxiv.org/abs/2304.01982).\n",
        "\n",
        "To run the code, please follow the instructions below. If you are using Colab or Kaggle Notebook, please use GPU or TPU resources for faster inference. For any bug or issue, please post an issue on our [Github](https://github.com/google-deepmind/xtr) or contact jinhyuklee@google.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p34LuD4Qz3G"
      },
      "source": [
        "## Install and Import Packages\n",
        "We need to install ScaNN for the efficient MIPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:19:17.762219Z",
          "iopub.status.busy": "2024-02-26T20:19:17.761862Z",
          "iopub.status.idle": "2024-02-26T20:19:35.168184Z",
          "shell.execute_reply": "2024-02-26T20:19:35.167348Z",
          "shell.execute_reply.started": "2024-02-26T20:19:17.762184Z"
        },
        "id": "_5Ca6GtEQz3G",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Use ScaNN for CPU/GPU\n",
        "!pip install pytrec_eval nltk tensorflow_text transformers scann\n",
        "\n",
        "# Use faiss for TPU\n",
        "# !pip install pytrec_eval nltk tensorflow_text transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:19:35.170113Z",
          "iopub.status.busy": "2024-02-26T20:19:35.169833Z",
          "iopub.status.idle": "2024-02-26T20:20:13.780264Z",
          "shell.execute_reply": "2024-02-26T20:20:13.779507Z",
          "shell.execute_reply.started": "2024-02-26T20:19:35.170083Z"
        },
        "id": "BJTa2rUDQz3H",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import collections\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import pytrec_eval\n",
        "import re\n",
        "import time\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import transformers\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "print('TF version:', tf.__version__)\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHQ-njp0Qz3I"
      },
      "source": [
        "## Load XTR\n",
        "Please set **XTR_MODEL** based on your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:20:13.781723Z",
          "iopub.status.busy": "2024-02-26T20:20:13.781214Z",
          "iopub.status.idle": "2024-02-26T20:20:13.789294Z",
          "shell.execute_reply": "2024-02-26T20:20:13.788678Z",
          "shell.execute_reply.started": "2024-02-26T20:20:13.781693Z"
        },
        "id": "eIeSZeH4Qz3I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class XTRModel(Enum):\n",
        "    BASE_EN = 1\n",
        "    BASE_EN_TPU = 2\n",
        "    BASE_MULTILINGUAL = 3\n",
        "    BASE_MULTILINGUAL_TPU = 4\n",
        "    XXL_EN = 5\n",
        "    XXL_EN_TPU = 6\n",
        "    XXL_MULTILINGUAL = 7\n",
        "    XXL_MULTILINGUAL_TPU = 8\n",
        "\n",
        "\n",
        "model_to_url = {\n",
        "    XTRModel.BASE_EN: \"/kaggle/input/xtr/tensorflow2/base-en/2/\",\n",
        "    XTRModel.BASE_EN_TPU: \"/kaggle/input/xtr/tensorflow2/base-en-tpu/2/\",\n",
        "    XTRModel.BASE_MULTILINGUAL: \"/kaggle/input/xtr/tensorflow2/base-multilingual/2/\",\n",
        "    XTRModel.BASE_MULTILINGUAL_TPU: \"/kaggle/input/xtr/tensorflow2/base-multilingual-tpu/2/\",\n",
        "    XTRModel.XXL_EN: \"/kaggle/input/xtr/tensorflow2/xxl-en/2/\",\n",
        "    XTRModel.XXL_EN_TPU: \"/kaggle/input/xtr/tensorflow2/xxl-en-tpu/2/\",\n",
        "    XTRModel.XXL_MULTILINGUAL: \"/kaggle/input/xtr/tensorflow2/xxl-multilingual/2/\",\n",
        "    XTRModel.XXL_MULTILINGUAL_TPU: \"/kaggle/input/xtr/tensorflow2/xxl-multilingual-tpu/2/\",\n",
        "}\n",
        "\n",
        "TPU_MODELS = [XTRModel.BASE_EN_TPU, XTRModel.BASE_MULTILINGUAL_TPU, XTRModel.XXL_EN_TPU, XTRModel.XXL_MULTILINGUAL_TPU]\n",
        "MULTILINGUAL_MODELS = [XTRModel.BASE_MULTILINGUAL, XTRModel.BASE_MULTILINGUAL_TPU, XTRModel.XXL_MULTILINGUAL, XTRModel.XXL_MULTILINGUAL_TPU]\n",
        "\n",
        "# Choose a model.\n",
        "XTR_MODEL = XTRModel.BASE_EN\n",
        "\n",
        "# XTR-related constants.\n",
        "MAX_SEQ_LEN = 512\n",
        "TOKEN_EMBED_DIM = 128\n",
        "\n",
        "# Other constants.\n",
        "DEBUG = False\n",
        "\n",
        "print(f\"You are using {XTR_MODEL}. Make sure that {XTR_MODEL} was added to the Notebook.\")\n",
        "if XTR_MODEL not in TPU_MODELS:\n",
        "    print('For CPU/GPU models, we recommend using the P100 accelerator.')\n",
        "else:\n",
        "    print('For TPU models, please set TPU as the accelerator.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:20:13.790862Z",
          "iopub.status.busy": "2024-02-26T20:20:13.790619Z",
          "iopub.status.idle": "2024-02-26T20:20:39.278998Z",
          "shell.execute_reply": "2024-02-26T20:20:39.278081Z",
          "shell.execute_reply.started": "2024-02-26T20:20:13.790825Z"
        },
        "id": "XhDT6ZMhQz3I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Loading Test\n",
        "# For CPU/GPU-based inference.\n",
        "if XTR_MODEL not in TPU_MODELS:\n",
        "    import scann\n",
        "    INDEX_TYPE = 'scann'\n",
        "    physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "    try:\n",
        "        for gpu in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(f\"set_memory_growth = True for {gpu}\")\n",
        "        if len(physical_devices) == 0:\n",
        "            print(\"Loading XTR on CPU.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "# For TPU-based inference.\n",
        "else:\n",
        "    import faiss\n",
        "    INDEX_TYPE = 'faiss'\n",
        "    try:\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        tf.config.experimental.enable_mlir_bridge()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "# Load XTR.\n",
        "model = tf.saved_model.load(model_to_url[XTR_MODEL])\n",
        "encoder = model.signatures[\"serving_default\"]\n",
        "\n",
        "# Test XTR encoding.\n",
        "sample_texts = tf.constant([\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"])\n",
        "sample_embeds = encoder(sample_texts)\n",
        "encodings = sample_embeds[\"encodings\"].numpy()\n",
        "mask = sample_embeds[\"mask\"].numpy()\n",
        "print(f\"encodings: {encodings.shape}, mask: {mask.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGng4aPiQz3J"
      },
      "source": [
        "## XTR Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:49:04.913667Z",
          "iopub.status.busy": "2024-02-26T20:49:04.913331Z",
          "iopub.status.idle": "2024-02-26T20:49:04.945511Z",
          "shell.execute_reply": "2024-02-26T20:49:04.944729Z",
          "shell.execute_reply.started": "2024-02-26T20:49:04.913638Z"
        },
        "id": "6bQPcr4YQz3J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title XTR architecture\n",
        "def profile(func, debug=DEBUG):\n",
        "    def wrap(*args, **kwargs):\n",
        "        started_at = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        if debug:\n",
        "          print(f\"{func.__name__} took {time.time() - started_at:.3f} seconds.\")\n",
        "        return result\n",
        "    return wrap\n",
        "\n",
        "\n",
        "class XTR(object):\n",
        "    def __init__(self, encoder, model_type, index_type='faiss'):\n",
        "        self.encoder = encoder\n",
        "        self.index_type = index_type  # must be 'faiss' or 'scann'. Otherwise uses bruteforce.\n",
        "\n",
        "        # Set the tokenizer based on the model type.\n",
        "        if model_type not in MULTILINGUAL_MODELS:\n",
        "            with tf.io.gfile.GFile(\"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\", \"rb\") as f:\n",
        "                self.tokenizer = text.SentencepieceTokenizer(model=f.read(), add_eos=True)\n",
        "        else:\n",
        "            with tf.io.gfile.GFile(\"gs://t5-data/vocabs/mc4.250000.100extra/sentencepiece.model\", \"rb\") as f:\n",
        "                self.tokenizer = text.SentencepieceTokenizer(model=f.read(), add_eos=True)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [self.tokenizer.id_to_string(id_).numpy().decode('utf-8') for id_ in self.tokenizer.tokenize(text)]\n",
        "\n",
        "    @profile\n",
        "    def get_token_embeddings(self, texts):\n",
        "        batch_embeds = self.encoder(tf.constant([t.lower() for t in texts]))\n",
        "        batch_lengths = np.sum(batch_embeds[\"mask\"].numpy(), axis=1)\n",
        "        return batch_embeds[\"encodings\"].cpu().numpy(), batch_lengths\n",
        "\n",
        "    @profile\n",
        "    def get_flatten_embeddings(self, batch_text, return_last_offset=False):\n",
        "        batch_embeddings, batch_lengths = self.get_token_embeddings(batch_text)\n",
        "        flatten_embeddings = None\n",
        "        num_tokens = 0\n",
        "        offsets = [0]\n",
        "        for batch_id, (embeddings, length) in enumerate(zip(batch_embeddings, batch_lengths)):\n",
        "            if flatten_embeddings is not None:\n",
        "                flatten_embeddings = np.append(flatten_embeddings, embeddings[:int(length)], axis=0)\n",
        "            else:\n",
        "                flatten_embeddings = embeddings[:int(length)]\n",
        "            num_tokens += int(length)\n",
        "            offsets.append(num_tokens)\n",
        "        assert num_tokens == flatten_embeddings.shape[0]\n",
        "        if not return_last_offset:\n",
        "            offsets = offsets[:-1]\n",
        "        return flatten_embeddings, offsets\n",
        "\n",
        "    @profile\n",
        "    def build_index(self, documents, batch_size=32):\n",
        "        all_token_embeds = np.zeros((len(documents)*MAX_SEQ_LEN, TOKEN_EMBED_DIM), dtype=np.float32)\n",
        "        all_doc_offsets = []\n",
        "        num_tokens = 0\n",
        "        for batch_idx in tqdm(range(0, len(documents), batch_size)):\n",
        "            batch_docs = documents[batch_idx:batch_idx+batch_size]\n",
        "            batch_embeds, batch_offsets = self.get_flatten_embeddings(batch_docs)\n",
        "            all_doc_offsets += [num_tokens + offset for offset in batch_offsets]\n",
        "            num_tokens += len(batch_embeds)\n",
        "            all_token_embeds[num_tokens-len(batch_embeds):num_tokens] = batch_embeds\n",
        "\n",
        "        # Use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher.\n",
        "        if self.index_type == 'scann':\n",
        "            self.searcher = scann.scann_ops_pybind.builder(all_token_embeds[:num_tokens], 10, \"dot_product\").tree(\n",
        "                num_leaves=min(2000, num_tokens), num_leaves_to_search=100, training_sample_size=min(250000, num_tokens)).score_ah(\n",
        "                1, anisotropic_quantization_threshold=0.1).build()\n",
        "        elif self.index_type == 'faiss':\n",
        "            ds = 128\n",
        "            num_clusters = 50\n",
        "            code_size = 64\n",
        "            quantizer = faiss.IndexFlatIP(ds)\n",
        "            opq_matrix = faiss.OPQMatrix(ds, code_size)\n",
        "            opq_matrix.niter = 10\n",
        "            sub_index = faiss.IndexIVFPQ(quantizer, ds, num_clusters, code_size, 4, faiss.METRIC_INNER_PRODUCT)\n",
        "            index = faiss.IndexPreTransform(opq_matrix, sub_index)\n",
        "            index.train(all_token_embeds[:num_tokens])\n",
        "            index.add(all_token_embeds[:num_tokens])\n",
        "            class FaissSearcher(object):\n",
        "                def __init__(self, index):\n",
        "                    self.index = index\n",
        "                def search_batched(self, query_embeds, final_num_neighbors, **kwargs):\n",
        "                    scores, top_ids = self.index.search(query_embeds, final_num_neighbors)\n",
        "                    return top_ids, scores\n",
        "            self.searcher = FaissSearcher(index)\n",
        "        # Used only for small-scale, exact inference.\n",
        "        else:\n",
        "            class BruteForceSearcher(object):\n",
        "                def search_batched(self, query_embeds, final_num_neighbors, **kwargs):\n",
        "                    scores = query_embeds.dot(all_token_embeds[:num_tokens].T) # Q x D\n",
        "                    top_ids = scores.argsort(axis=1)[:, ::-1][:,:final_num_neighbors] # Q x top_k\n",
        "                    return top_ids, [q_score[q_top_ids] for q_score, q_top_ids in zip(scores, top_ids)] # (Q x top_k, Q x top_k)\n",
        "            self.searcher = BruteForceSearcher()\n",
        "\n",
        "        self.doc_offsets = all_doc_offsets\n",
        "        self.doc_offsets.append(num_tokens)  # Add final number of tokens.\n",
        "        self.tid2did = {\n",
        "            self.doc_offsets[did] + tid: did\n",
        "            for did in range(len(self.doc_offsets)-1)\n",
        "            for tid in range(self.doc_offsets[did+1] - self.doc_offsets[did])\n",
        "        }\n",
        "        self.tid2did[-1] = 0\n",
        "        self.docs = documents\n",
        "        print(\"Index Ready!\", self.searcher)\n",
        "\n",
        "    @profile\n",
        "    def batch_search_tokens(self, batch_query, token_top_k=100, leaves_to_search=100, pre_reorder_num_neighbors=100):\n",
        "        all_query_encodings, query_offsets = self.get_flatten_embeddings(batch_query, return_last_offset=True)\n",
        "        all_neighbors, all_scores = self.searcher.search_batched(\n",
        "            all_query_encodings, final_num_neighbors=token_top_k, leaves_to_search=leaves_to_search, pre_reorder_num_neighbors=pre_reorder_num_neighbors\n",
        "        )\n",
        "        return [\n",
        "            (\n",
        "                [f'q_{i}' for i in range(query_offsets[oid], query_offsets[oid+1])],  # query_id\n",
        "                all_neighbors[query_offsets[oid]:query_offsets[oid+1]],  # neighbors\n",
        "                all_scores[query_offsets[oid]:query_offsets[oid+1]],  # scores\n",
        "            )\n",
        "            for oid in range(len(query_offsets)-1)\n",
        "        ]\n",
        "\n",
        "    @profile\n",
        "    def estimate_missing_similarity(self, batch_result):\n",
        "        batch_qtoken_to_ems = [dict() for _ in range(len(batch_result))]\n",
        "        for b_idx, (query_tokens, _, distances) in enumerate(batch_result):\n",
        "            for token_idx, qtoken in enumerate(query_tokens):\n",
        "                idx_t = (token_idx, qtoken)\n",
        "                # Use similarity of the last token as imputed similarity.\n",
        "                batch_qtoken_to_ems[b_idx][idx_t] = distances[token_idx][-1]\n",
        "        return batch_qtoken_to_ems\n",
        "\n",
        "    def aggregate_scores(self, batch_result, batch_ems, document_top_k):\n",
        "        \"\"\"Aggregates token-level retrieval scores into query-document scores.\"\"\"\n",
        "\n",
        "        @profile\n",
        "        def get_did2scores(query_tokens, all_neighbors, all_scores):\n",
        "            did2scores = {}\n",
        "            # |Q| x k'\n",
        "            for qtoken_idx, (qtoken, neighbors, scores) in enumerate(zip(query_tokens, all_neighbors, all_scores)):\n",
        "                for _, (doc_token_id, score) in enumerate(zip(neighbors, scores)):\n",
        "                    if np.isnan(score):\n",
        "                        continue\n",
        "                    docid = self.tid2did[doc_token_id]\n",
        "                    if docid not in did2scores:\n",
        "                        did2scores[docid] = {}\n",
        "                    qtoken_with_idx = (qtoken_idx, qtoken)\n",
        "                    if qtoken_with_idx not in did2scores[docid]:\n",
        "                        # Only keep the top score for sum-of-max.\n",
        "                        did2scores[docid][qtoken_with_idx] = score\n",
        "\n",
        "            return did2scores\n",
        "        batch_did2scores = [get_did2scores(qtokens, neighbors, scores) for qtokens, neighbors, scores in batch_result]\n",
        "\n",
        "        @profile\n",
        "        def add_ems(did2scores, query_tokens, ems):\n",
        "            # |Q| x |Q|k' (assuming most docid is unique)\n",
        "            for qtoken_idx, qtoken in enumerate(query_tokens):\n",
        "                qtoken_with_idx = (qtoken_idx, qtoken)\n",
        "                for docid, scores in did2scores.items():\n",
        "                    if qtoken_with_idx not in scores:\n",
        "                        scores[qtoken_with_idx] = ems[qtoken_with_idx]\n",
        "        for did2scores, result, ems in zip(batch_did2scores, batch_result, batch_ems):\n",
        "            add_ems(did2scores, result[0], ems)\n",
        "\n",
        "        @profile\n",
        "        def get_final_score(did2scores, query_tokens):\n",
        "            final_qd_score = {}\n",
        "            # |Q|k' x |Q|\n",
        "            for docid, scores in did2scores.items():\n",
        "                assert len(scores) == len(query_tokens)\n",
        "                final_qd_score[docid] = sum(scores.values()) / len(scores)\n",
        "            return final_qd_score\n",
        "\n",
        "        batch_scores = [get_final_score(did2scores, result[0]) for did2scores, result in zip(batch_did2scores, batch_result)]\n",
        "\n",
        "        batch_ranking = [\n",
        "            sorted([(docid, score) for docid, score in final_qd_score.items()], key=lambda x: x[1], reverse=True)[:document_top_k]\n",
        "            for final_qd_score in batch_scores\n",
        "        ]\n",
        "        return batch_ranking\n",
        "\n",
        "    def get_document_text(self, batch_ranking):\n",
        "        batch_retrieved_docs = []\n",
        "        for ranking in batch_ranking:\n",
        "            retrieved_docs = []\n",
        "            for did, score in ranking:\n",
        "                retrieved_docs.append((did, score, self.docs[did]))\n",
        "            batch_retrieved_docs.append(retrieved_docs)\n",
        "        return batch_retrieved_docs\n",
        "\n",
        "    def retrieve_docs(\n",
        "        self,\n",
        "        batch_query: List[str],\n",
        "        token_top_k: int = 100,\n",
        "        leaves_to_search: int = 100,\n",
        "        pre_reorder_num_neighbors: int = 100,\n",
        "        document_top_k: int = 100,\n",
        "        return_text: bool = True,\n",
        "    ):\n",
        "        \"\"\"Runs XTR retrieval for a query.\"\"\"\n",
        "        batch_result = self.batch_search_tokens(batch_query, token_top_k=token_top_k, leaves_to_search=leaves_to_search, pre_reorder_num_neighbors=pre_reorder_num_neighbors)\n",
        "        batch_mae = self.estimate_missing_similarity(batch_result)\n",
        "        batch_ranking = self.aggregate_scores(batch_result, batch_mae, document_top_k)\n",
        "        if return_text:\n",
        "            return self.get_document_text(batch_ranking), batch_result\n",
        "        else:\n",
        "            return batch_ranking, batch_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeGKwRhrQz3K"
      },
      "source": [
        "## Sample Run\n",
        "For a sample corpus, we use sentences from Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.execute_input": "2024-02-26T20:49:06.279358Z",
          "iopub.status.busy": "2024-02-26T20:49:06.279009Z",
          "iopub.status.idle": "2024-02-26T20:49:06.293516Z",
          "shell.execute_reply": "2024-02-26T20:49:06.292766Z",
          "shell.execute_reply.started": "2024-02-26T20:49:06.279327Z"
        },
        "id": "IVtuEJzVQz3K",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/Google\n",
        "sample_doc = \"\"\"Google LLC (/ˈɡuːɡəl/ (listen)) is an American multinational technology company focusing on online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, artificial intelligence,[9] and consumer electronics. It has been referred to as \"the most powerful company in the world\"[10] and one of the world's most valuable brands due to its market dominance, data collection, and technological advantages in the area of artificial intelligence.[11][12][13] Its parent company Alphabet is considered one of the Big Five American information technology companies, alongside Amazon, Apple, Meta, and Microsoft.\n",
        "Google was founded on September 4, 1998, by computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California. Together they own about 14% of its publicly listed shares and control 56% of its stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet.[14]\n",
        "The company has since rapidly grown to offer a multitude of products and services beyond Google Search, many of which hold dominant market positions. These products address a wide range of use cases, including email (Gmail), navigation (Waze \u0026 Maps), cloud computing (Cloud), web browsing (Chrome), video sharing (YouTube), productivity (Workspace), operating systems (Android), cloud storage (Drive), language translation (Translate), photo storage (Photos), video calling (Meet), smart home (Nest), smartphones (Pixel), wearable technology (Pixel Watch \u0026 Fitbit), music streaming (YouTube Music), video on demand (YouTube TV), artificial intelligence (Google Assistant), machine learning APIs (TensorFlow), AI chips (TPU), and more. Discontinued Google products include gaming (Stadia), Glass, Google+, Reader, Play Music, Nexus, Hangouts, and Inbox by Gmail.[15][16]\n",
        "Google's other ventures outside of Internet services and consumer electronics include quantum computing (Sycamore), self-driving cars (Waymo, formerly the Google Self-Driving Car Project), smart cities (Sidewalk Labs), and transformer models (Google Brain).[17]\n",
        "Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter. Google is also the largest search engine, mapping and navigation application, email provider, office suite, video sharing platform, photo and cloud storage provider, mobile operating system, web browser, ML framework, and AI virtual assistant provider in the world as measured by market share. On the list of most valuable brands, Google is ranked second by Forbes[18] and fourth by Interbrand.[19] It has received significant criticism involving issues such as privacy concerns, tax avoidance, censorship, search neutrality, antitrust and abuse of its monopoly position.\n",
        "Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in California.[20][21][22] The project initially involved an unofficial \"third founder\", Scott Hassan, the original lead programmer who wrote much of the code for the original Google Search engine, but he left before Google was officially founded as a company;[23][24] Hassan went on to pursue a career in robotics and founded the company Willow Garage in 2006.[25][26]\n",
        "While conventional search engines ranked results by counting how many times the search terms appeared on the page, they theorized about a better system that analyzed the relationships among websites.[27] They called this algorithm PageRank; it determined a website's relevance by the number of pages, and the importance of those pages that linked back to the original site.[28][29] Page told his ideas to Hassan, who began writing the code to implement Page's ideas.[23]\n",
        "Page and Brin originally nicknamed the new search engine \"BackRub\", because the system checked backlinks to estimate the importance of a site.[20][30][31] Hassan as well as Alan Steremberg were cited by Page and Brin as being critical to the development of Google. Rajeev Motwani and Terry Winograd later co-authored with Page and Brin the first paper about the project, describing PageRank and the initial prototype of the Google search engine, published in 1998. Héctor García-Molina and Jeff Ullman were also cited as contributors to the project.[32] PageRank was influenced by a similar page-ranking and site-scoring algorithm earlier used for RankDex, developed by Robin Li in 1996, with Larry Page's PageRank patent including a citation to Li's earlier RankDex patent; Li later went on to create the Chinese search engine Baidu.[33][34]\n",
        "Eventually, they changed the name to Google; the name of the search engine was a misspelling of the word googol,[20][35][36] a very large number written 10100 (1 followed by 100 zeros), picked to signify that the search engine was intended to provide large quantities of information.[37]\n",
        "Google was initially funded by an August 1998 investment of $100,000 from Andy Bechtolsheim,[20] co-founder of Sun Microsystems. This initial investment served as a motivation to incorporate the company to be able to use the funds.[39][40] Page and Brin initially approached David Cheriton for advice because he had a nearby office in Stanford, and they knew he had startup experience, having recently sold the company he co-founded, Granite Systems, to Cisco for $220 million. David arranged a meeting with Page and Brin and his Granite co-founder Andy Bechtolsheim. The meeting was set for 8 AM at the front porch of David's home in Palo Alto and it had to be brief because Andy had another meeting at Cisco, where he now worked after the acquisition, at 9 AM. Andy briefly tested a demo of the website, liked what he saw, and then went back to his car to grab the check. David Cheriton later also joined in with a $250,000 investment.[41][42]\n",
        "Google received money from two other angel investors in 1998: Amazon.com founder Jeff Bezos, and entrepreneur Ram Shriram.[43] Page and Brin had first approached Shriram, who was a venture capitalist, for funding and counsel, and Shriram invested $250,000 in Google in February 1998. Shriram knew Bezos because Amazon had acquired Junglee, at which Shriram was the president. It was Shriram who told Bezos about Google. Bezos asked Shriram to meet Google's founders and they met 6 months after Shriram had made his investment when Bezos and his wife were in a vacation trip to the Bay Area. Google's initial funding round had already formally closed but Bezos' status as CEO of Amazon was enough to persuade Page and Brin to extend the round and accept his investment.[44][45]\n",
        "Between these initial investors, friends, and family Google raised around $1,000,000, which is what allowed them to open up their original shop in Menlo Park, California.[46] Craig Silverstein, a fellow PhD student at Stanford, was hired as the first employee.[22][47][48]\n",
        "After some additional, small investments through the end of 1998 to early 1999,[43] a new $25 million round of funding was announced on June 7, 1999,[49] with major investors including the venture capital firms Kleiner Perkins and Sequoia Capital.[40] Both firms were initially reticent about investing jointly in Google, as each wanted to retain a larger percentage of control over the company to themselves. Larry and Sergey however insisted in taking investments from both. Both venture companies finally agreed to investing jointly $12.5 million each due to their belief in Google's great potential and through mediation of earlier angel investors Ron Conway and Ram Shriram who had contacts in the venture companies.[50]\n",
        "In March 1999, the company moved its offices to Palo Alto, California,[51] which is home to several prominent Silicon Valley technology start-ups.[52] The next year, Google began selling advertisements associated with search keywords against Page and Brin's initial opposition toward an advertising-funded search engine.[53][22] To maintain an uncluttered page design, advertisements were solely text-based.[54] In June 2000, it was announced that Google would become the default search engine provider for Yahoo!, one of the most popular websites at the time, replacing Inktomi.[55][56]\n",
        "In 2003, after outgrowing two other locations, the company leased an office complex from Silicon Graphics, at 1600 Amphitheatre Parkway in Mountain View, California.[58] Three years later, Google bought the property from SGI for $319 million.[59] By that time, the name \"Google\" had found its way into everyday language, causing the verb \"google\" to be added to the Merriam-Webster Collegiate Dictionary and the Oxford English Dictionary, denoted as: \"to use the Google search engine to obtain information on the Internet\".[60][61] The first use of the verb on television appeared in an October 2002 episode of Buffy the Vampire Slayer.[62]\n",
        "Additionally, in 2001 Google's investors felt the need to have a strong internal management, and they agreed to hire Eric Schmidt as the chairman and CEO of Google.[46] Eric was proposed by John Doerr from Kleiner Perkins. He had been trying to find a CEO that Sergey and Larry would accept for several months, but they rejected several candidates because they wanted to retain control over the company. Michael Moritz from Sequoia Capital at one point even menaced requesting Google to immediately pay back Sequoia's $12.5m investment if they did not fulfill their promise to hire a chief executive officer, which had been made verbally during investment negotiations. Eric wasn't initially enthusiastic about joining Google either, as the company's full potential hadn't yet been widely recognized at the time, and as he was occupied with his responsibilities at Novell where he was CEO. As part of him joining, Eric agreed to buy $1 million of Google preferred stocks as a way to show his commitment and to provide funds Google needed.[63]\n",
        "On August 19, 2004, Google became a public company via an initial public offering. At that time Larry Page, Sergey Brin, and Eric Schmidt agreed to work together at Google for 20 years, until the year 2024.[64] The company offered 19,605,052 shares at a price of $85 per share.[65][66] Shares were sold in an online auction format using a system built by Morgan Stanley and Credit Suisse, underwriters for the deal.[67][68] The sale of $1.67 billion gave Google a market capitalization of more than $23 billion.[69]\n",
        "On November 13, 2006, Google acquired YouTube for $1.65 billion in Google stock,[70][71][72][73] On March 11, 2008, Google acquired DoubleClick for $3.1 billion, transferring to Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.[74][75]\n",
        "By 2011, Google was handling approximately 3 billion searches per day. To handle this workload, Google built 11 data centers around the world with several thousand servers in each. These data centers allowed Google to handle the ever-changing workload more efficiently.[46]\n",
        "In May 2011, the number of monthly unique visitors to Google surpassed one billion for the first time.[76][77]\n",
        "In May 2012, Google acquired Motorola Mobility for $12.5 billion, in its largest acquisition to date.[78][79][80] This purchase was made in part to help Google gain Motorola's considerable patent portfolio on mobile phones and wireless technologies, to help protect Google in its ongoing patent disputes with other companies,[81] mainly Apple and Microsoft,[82] and to allow it to continue to freely offer Android.[83]\n",
        "\"\"\"\n",
        "sample_doc = re.sub(r'\\[\\d+\\]', '', sample_doc)\n",
        "\n",
        "# Single-sentence chunks.\n",
        "chunks = [chunk.lower() for chunk in sent_tokenize(sample_doc)]\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f'chunk{i}: {chunk[:150]} \\n')\n",
        "    if i \u003e 3:\n",
        "        print('...\\n')\n",
        "        break\n",
        "print('total # of chunks:', len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYvut9uQz3K"
      },
      "source": [
        "Now, let's run XTR!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:51:41.719864Z",
          "iopub.status.busy": "2024-02-26T20:51:41.718614Z",
          "iopub.status.idle": "2024-02-26T20:51:42.454281Z",
          "shell.execute_reply": "2024-02-26T20:51:42.452368Z",
          "shell.execute_reply.started": "2024-02-26T20:51:41.719821Z"
        },
        "id": "C4iBAPGhQz3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "xtr = XTR(encoder=encoder, model_type=XTR_MODEL, index_type='brute')\n",
        "xtr.build_index(chunks)\n",
        "\n",
        "query = \"Who founded google\" if XTR_MODEL not in MULTILINGUAL_MODELS else \"구글 창립자\"\n",
        "retrieved_docs, metadata = xtr.retrieve_docs([query], document_top_k=3)\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "for rank, (did, score, doc) in enumerate(retrieved_docs[0]):\n",
        "    print(f\"[{rank}] doc={did} ({score:.3f}): {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8uBmi3Qz3L"
      },
      "source": [
        "## Analyze Token Retrieval of XTR\n",
        "We can check the token-level retrieval results happening in XTR. For each query token, we show its retrieved document tokens from the corpus in red. Similar analysis was done in the [paper](https://arxiv.org/pdf/2304.01982.pdf) in the qualitative analysis section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:51:44.280598Z",
          "iopub.status.busy": "2024-02-26T20:51:44.280253Z",
          "iopub.status.idle": "2024-02-26T20:51:44.738677Z",
          "shell.execute_reply": "2024-02-26T20:51:44.737865Z",
          "shell.execute_reply.started": "2024-02-26T20:51:44.280566Z"
        },
        "id": "0OlTerL-Qz3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Analysis\n",
        "BOLD = '\\033[91m\\033[01m'\n",
        "END = '\\033[0m\\033[0m'\n",
        "FRONT_TOKEN_CONTEXT = 10\n",
        "BACK_TOKEN_CONTEXT = 30\n",
        "\n",
        "\n",
        "def smart_join(token_list):\n",
        "    res = ''\n",
        "    for token in token_list:\n",
        "        if token.startswith('▁'):\n",
        "            if len(res) \u003e 0:\n",
        "                res += ' '\n",
        "            res += token\n",
        "        else:\n",
        "            res += token\n",
        "    return res\n",
        "\n",
        "\n",
        "def xtr_pretty_print(query, metadata):\n",
        "    print(f'Given a query \"{query}\", XTR retrieves the following document tokens for each query token:')\n",
        "    qtokens = xtr.tokenize(query.lower())\n",
        "    for qidx, (qtoken, nns, scores) in enumerate(zip(metadata[0][0], metadata[0][1], metadata[0][2])):\n",
        "        print(f\"====={qtoken}:{BOLD + qtokens[qidx] + END}======\")\n",
        "        top_retrieved_tokens = [(int(tid), s) for tid, s in sorted([(nn, score) for nn, score in zip(nns, scores)], key=lambda v: -v[1])[:5]]\n",
        "        for tid, s in top_retrieved_tokens:\n",
        "            did = xtr.tid2did[tid]\n",
        "            new_tid = tid - xtr.doc_offsets[did]\n",
        "            curr_doc = xtr.tokenize(xtr.docs[did])\n",
        "            print(f'[{s:.3f}]', f'{\"... \" if new_tid \u003e 0 else \"\"}' + smart_join(curr_doc[max(0, new_tid-FRONT_TOKEN_CONTEXT):new_tid] + [BOLD, curr_doc[new_tid], END] + curr_doc[new_tid+1:new_tid+BACK_TOKEN_CONTEXT]) + ' ...')\n",
        "\n",
        "xtr_pretty_print(query, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC8a291XQz3L"
      },
      "source": [
        "## Load BEIR Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:34:14.012407Z",
          "iopub.status.busy": "2024-02-26T20:34:14.011725Z",
          "iopub.status.idle": "2024-02-26T20:34:42.830185Z",
          "shell.execute_reply": "2024-02-26T20:34:42.828832Z",
          "shell.execute_reply.started": "2024-02-26T20:34:14.012369Z"
        },
        "id": "opVkgFmIQz3L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Install the beir PyPI package\n",
        "!pip install beir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:34:42.832447Z",
          "iopub.status.busy": "2024-02-26T20:34:42.832104Z",
          "iopub.status.idle": "2024-02-26T20:34:44.080040Z",
          "shell.execute_reply": "2024-02-26T20:34:44.079191Z",
          "shell.execute_reply.started": "2024-02-26T20:34:42.832409Z"
        },
        "id": "0fhb5-X-Qz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Load Scifact from BEIR\n",
        "from beir import util\n",
        "\n",
        "# Supports two smallest datasets from BEIR\n",
        "class BEIR(Enum):\n",
        "    SCIFACT = 1\n",
        "    NFCORPUS = 2\n",
        "\n",
        "beir_datasets = {\n",
        "    BEIR.SCIFACT: \"scifact\",\n",
        "    BEIR.NFCORPUS: \"nfcorpus\"\n",
        "}\n",
        "\n",
        "# Choose a dataset.\n",
        "DATASET = BEIR.SCIFACT\n",
        "\n",
        "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(beir_datasets[DATASET])\n",
        "out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
        "data_path = util.download_and_unzip(url, out_dir)\n",
        "print(\"Dataset downloaded here: {}\".format(data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:34:44.081292Z",
          "iopub.status.busy": "2024-02-26T20:34:44.081012Z",
          "iopub.status.idle": "2024-02-26T20:34:44.482337Z",
          "shell.execute_reply": "2024-02-26T20:34:44.481093Z",
          "shell.execute_reply.started": "2024-02-26T20:34:44.081264Z"
        },
        "id": "0_xCFu7SQz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!ls datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:34:44.485083Z",
          "iopub.status.busy": "2024-02-26T20:34:44.484754Z",
          "iopub.status.idle": "2024-02-26T20:34:44.570924Z",
          "shell.execute_reply": "2024-02-26T20:34:44.570159Z",
          "shell.execute_reply.started": "2024-02-26T20:34:44.485024Z"
        },
        "id": "YuibtZtIQz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "\n",
        "data_path = f\"datasets/{beir_datasets[DATASET]}\"\n",
        "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\") # or split = \"train\" or \"dev\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGRtHeROQz3M"
      },
      "source": [
        "## Index BEIR Corpus\n",
        "For Scifact + XTR-base-en (P100), this should take about 3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:52:09.386443Z",
          "iopub.status.busy": "2024-02-26T20:52:09.386121Z",
          "iopub.status.idle": "2024-02-26T20:53:08.364296Z",
          "shell.execute_reply": "2024-02-26T20:53:08.363187Z",
          "shell.execute_reply.started": "2024-02-26T20:52:09.386415Z"
        },
        "id": "NKwso1rwQz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "all_docs = []\n",
        "all_keys = []\n",
        "for doc_key, doc in tqdm(corpus.items()):\n",
        "    doc_text = f\"{doc['title']} {doc['text']}\".lower()\n",
        "    all_keys.append(doc_key)\n",
        "    all_docs.append(doc_text)\n",
        "\n",
        "xtr = XTR(encoder=encoder, model_type=XTR_MODEL, index_type=INDEX_TYPE)\n",
        "xtr.build_index(all_docs)\n",
        "print(f\"XTR Index Size: {len(xtr.tid2did)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc7VMMvJQz3M"
      },
      "source": [
        "## Run BEIR Evaluation\n",
        "For Scifact, XTR-base-en (P100), this should take about 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:55:44.531414Z",
          "iopub.status.busy": "2024-02-26T20:55:44.531072Z",
          "iopub.status.idle": "2024-02-26T20:55:44.536572Z",
          "shell.execute_reply": "2024-02-26T20:55:44.535739Z",
          "shell.execute_reply.started": "2024-02-26T20:55:44.531385Z"
        },
        "id": "alZe6Fd-Qz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluation hyperparameters.\n",
        "TOKEN_TOP_K = 1000\n",
        "NUM_LEAVES = 100  # ScaNN-related.\n",
        "TREC_TOP_K = 100\n",
        "\n",
        "# NOTE: If .reorder(100) was used for the ScaNN searcher, pre_reorder_num_neighbors\n",
        "# should be as large as token_top_k. If not, searcher will ignore this.\n",
        "NUM_REORDER = 1000\n",
        "assert NUM_REORDER \u003e= TOKEN_TOP_K, \"NUM_REORDER needs to be as large as TOKEN_TOP_K\"\n",
        "\n",
        "if INDEX_TYPE == 'faiss':\n",
        "    sub_index = faiss.extract_index_ivf(xtr.searcher.index)\n",
        "    sub_index.nprobe = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-26T20:55:44.750564Z",
          "iopub.status.busy": "2024-02-26T20:55:44.750237Z",
          "iopub.status.idle": "2024-02-26T20:57:18.294908Z",
          "shell.execute_reply": "2024-02-26T20:57:18.293957Z",
          "shell.execute_reply.started": "2024-02-26T20:55:44.750533Z"
        },
        "id": "9cqN6LPiQz3M",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "predictions = {}\n",
        "# Running evaluation per query for a better latency measurement.\n",
        "for q_idx, (query_key, query) in tqdm(enumerate(queries.items()), total=len(queries)):\n",
        "    ranking, metadata = xtr.retrieve_docs(\n",
        "        [query.lower()],\n",
        "        token_top_k=TOKEN_TOP_K,\n",
        "        leaves_to_search=NUM_LEAVES,\n",
        "        pre_reorder_num_neighbors=NUM_REORDER,  # does not have any effect since the default searcher does not use reorder.\n",
        "        return_text=False\n",
        "    )\n",
        "    ranking = ranking[0]\n",
        "    predictions[query_key] = {str(all_keys[did]): score for did, score in ranking[:TREC_TOP_K]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LayJFpWjQz3M"
      },
      "source": [
        "For a reference, XTR-base-en on Scifact scores **71.7% nDCG@10 and 93.1% Recall@100** with brute-force search as described in the [paper](https://arxiv.org/pdf/2304.01982.pdf) (See Table E.1 when k'=1000).\n",
        "\n",
        "The performance below shows a result with an approximate search with ScaNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-02-26T20:57:21.053288Z",
          "iopub.status.busy": "2024-02-26T20:57:21.052858Z",
          "iopub.status.idle": "2024-02-26T20:57:21.087647Z",
          "shell.execute_reply": "2024-02-26T20:57:21.086754Z",
          "shell.execute_reply.started": "2024-02-26T20:57:21.053251Z"
        },
        "id": "3EQETRi7Qz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run pytrec_eval.\n",
        "K_VALUES = [5, 10, 50, 100]\n",
        "METRIC_NAMES = ['ndcg_cut', 'map_cut', 'recall']\n",
        "\n",
        "def eval_metrics(qrels, predictions):\n",
        "    measurements = []\n",
        "    for metric_name in METRIC_NAMES:\n",
        "        measurements.append(\n",
        "            f\"{metric_name}.\" + \",\".join([str(k) for k in K_VALUES])\n",
        "        )\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, measurements)\n",
        "    final_scores = evaluator.evaluate(predictions)\n",
        "\n",
        "    final_metrics = dict()\n",
        "    for metric_name in METRIC_NAMES:\n",
        "        for k in K_VALUES:\n",
        "            final_metrics[f\"{metric_name}@{k}\"] = 0.0\n",
        "\n",
        "    for query_id in final_scores.keys():\n",
        "        for metric_name in METRIC_NAMES:\n",
        "            for k in K_VALUES:\n",
        "                final_metrics[f\"{metric_name}@{k}\"] += final_scores[query_id][\n",
        "                    f\"{metric_name}_{k}\"\n",
        "                ]\n",
        "\n",
        "    for metric_name in METRIC_NAMES:\n",
        "        for k in K_VALUES:\n",
        "            final_metrics[f\"{metric_name}@{k}\"] = round(\n",
        "                final_metrics[f\"{metric_name}@{k}\"] / len(final_scores), 5\n",
        "            )\n",
        "\n",
        "    print(\"[Result]\")\n",
        "    for metric_name, metric_score in final_metrics.items():\n",
        "        print(f\"{metric_name}: {metric_score:.4f}\")\n",
        "\n",
        "eval_metrics(qrels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXK81_VEQz3N"
      },
      "source": [
        "## Load MIRACL Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8unEwDFOQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-L2TwWOQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Please get the huggingface user access token from https://huggingface.co/settings/tokens\n",
        "# git clone https://{huggingface_id}:{huggingface_user_access_tokens}@huggingface.co/datasets/miracl/miracl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbgvCQcEQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "qrels = {}\n",
        "lang='sw'  # or choose any of the 16 languages\n",
        "with open(f\"miracl/miracl-v1.0-{lang}/qrels/qrels.miracl-v1.0-{lang}-dev.tsv\") as file:\n",
        "    qrels_tsv = csv.reader(file, delimiter=\"\\t\")\n",
        "    for line in qrels_tsv:\n",
        "        qid, _, did, judgement = line\n",
        "        if qid not in qrels:\n",
        "            qrels[qid] = {}\n",
        "        qrels[qid][did] = int(judgement)\n",
        "\n",
        "print(f\"{len(qrels)} qrels loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP8y1JjgQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "topics = {}  # same as query\n",
        "with open(f\"miracl/miracl-v1.0-{lang}/topics/topics.miracl-v1.0-{lang}-dev.tsv\") as file:\n",
        "    topics_tsv = csv.reader(file, delimiter=\"\\t\")\n",
        "    for line in topics_tsv:\n",
        "        qid, query = line\n",
        "        topics[qid] = query\n",
        "\n",
        "print(f\"{len(topics)} topics loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvcc2JnnQz3N"
      },
      "source": [
        "## Index MIRACL Corpus\n",
        "Since the corpus is large, we recommend using TPUs for the corpus encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxrNX3edQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "miracl_corpus = datasets.load_dataset('miracl/miracl-corpus', lang)['train']\n",
        "all_keys = []\n",
        "all_docs = []\n",
        "for doc in tqdm(miracl_corpus):\n",
        "    doc_text = f\"{doc['title']} {doc['text']}\".lower()\n",
        "    all_keys.append(doc['docid'])\n",
        "    all_docs.append(doc_text)\n",
        "\n",
        "print(f\"Sample: {all_docs[0]}\")\n",
        "\n",
        "xtr = XTR(encoder=encoder, model_type=XTR_MODEL, index_type=INDEX_TYPE)\n",
        "xtr.build_index(all_docs)\n",
        "print(f\"XTR Index Size: {len(xtr.tid2did)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6vfa_oHQz3N"
      },
      "source": [
        "## Run MIRACL Evaluation\n",
        "For a reference, mXTR-base on MIRACL-Swahili scores **69.7% nDCG@10** with brute-force search as described in the [paper](https://arxiv.org/pdf/2304.01982.pdf) (See Table 4).\n",
        "\n",
        "\n",
        "The performance below shows a result with an approximate search with ScaNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js1keogMQz3N"
      },
      "outputs": [],
      "source": [
        "# Evaluation hyperparameters.\n",
        "TOKEN_TOP_K = 1000\n",
        "NUM_LEAVES = 100  # ScaNN-related.\n",
        "TREC_TOP_K = 100\n",
        "\n",
        "# NOTE: If .reorder(100) was used for the ScaNN searcher, pre_reorder_num_neighbors\n",
        "# should be as large as token_top_k. If not, searcher will ignore this.\n",
        "NUM_REORDER = 1000\n",
        "assert NUM_REORDER \u003e= TOKEN_TOP_K, \"NUM_REORDER needs to be as large as TOKEN_TOP_K\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBJBe9DfQz3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "predictions = {}\n",
        "# Running evaluation per query for a better latency measurement.\n",
        "for q_idx, (query_key, query) in tqdm(enumerate(topics.items()), total=len(topics)):\n",
        "    ranking, metadata = xtr.retrieve_docs(\n",
        "        [query.lower()],\n",
        "        token_top_k=TOKEN_TOP_K,\n",
        "        leaves_to_search=NUM_LEAVES,\n",
        "        pre_reorder_num_neighbors=NUM_REORDER,  # No effect.\n",
        "        return_text=True\n",
        "    )\n",
        "    ranking = ranking[0]\n",
        "    predictions[query_key] = {str(all_keys[did]): score for did, score, _ in ranking[:TREC_TOP_K]}\n",
        "\n",
        "eval_metrics(qrels, predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TGng4aPiQz3J"
      ],
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1EDHRFjYWD7UzKg0JbgK1SYgv436G1XFq",
          "timestamp": 1709068827285
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
